

<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

 


      <title>机器学习12种分类模型的性能对比 - </title>

  <meta name="description" content="接上回，我们已经知道了机器学习分类的本质，那么接下来就是要介绍机器学习中的分类模型的各种性能了。
简介
我们将采用12种分类模型，分别是：GaussianNB、MultinomialNB、KNNeighbors、SVC、DecisionTree、RandomForest、GradientBoosting、LGBM、XGB、CatBoost、AdaBoost、MLP，用这些模型对数据集进行训练，然后对测试集进行预测，最后将预测结果与真实结果进行对比，得到各个模型的性能。
数据集
我们采用以下代码生成随机分类数据：
x,y = make_classification(n_samples=100000,n_features=3,n_classes=4,n_informative=3,n_redundant=0,random_state=50,n_clusters_per_class=1)
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1)
这里能够生成100000个样本，每个样本有3个特征，4个类别，3个特征是有效特征，0个冗余特征，随机种子为50，每个类别有1个簇。然后我们将数据集分为训练集和测试集，测试集占10%.
由于每个样本都是一个3维向量，所以我们将数据取前2维数据将数据集可视化，如下图所示：

评估函数
我们采用准确率(Accuracy)、精确率(Precision)、召回率(Recall)、F1值(F1-score)、R2值(R2-score)，这5个指标来评估模型的性能。
准确率(Accuracy)
准确率是指分类正确的样本数占总样本数的比例，即：
$Accuracy = \frac{TP&#43;TN}{TP&#43;TN&#43;FP&#43;FN}$
其中，TP是真正例，TN是真负例，FP是假正例，FN是假负例。
精确率(Precision)
精确率是指分类正确的正例数占分类为正例的样本数的比例，即：

$$
Precision = \frac{TP}{TP&#43;FP}
$$

其中，TP是真正例，FP是假正例。
召回率(Recall)
召回率是指分类正确的正例数占真正例的比例，即：
$Recall = \frac{TP}{TP&#43;FN}$
其中，TP是真正例，FN是假负例。
F1值(F1-score)
F1值是精确率和召回率的调和平均数，即：

$$
F1 = \frac{2*Precision*Recall}{Precision&#43;Recall}
$$

F1值越接近1，表明模型的性能越好。
R2值(R2-score)
R2值是指预测值与真实值的相关系数，即：

$$
R2 = 1-\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}
$$

其中，$y_i$是真实值，$\hat{y_i}$是预测值，$\bar{y}$是真实值的均值。
R2值越接近1，表明模型的性能越好。
各模型表现
1. GaussianNB

其中GaussianNB的参数priors为None，var_smoothing为1e-09，表明我们没有对先验概率进行设置，而且我们对方差进行了平滑处理。
2. MultinomialNB

其中MultinomialNB的参数alpha为1.0，fit_prior为True，class_prior为None，表明我们对先验概率进行了设置，而且我们对先验概率进行了平滑处理。
3. KNNeighbors

其中KNNeighbors的参数n_neighbors为30，表明我们对最近邻的个数为30。
4. SVC

5. DecisionTree

6. RandomForest
"><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "Juiceright",
    
    "url": "https:\/\/cywd123.github.io\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/cywd123.github.io\/"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https:\/\/cywd123.github.io\/",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https:\/\/cywd123.github.io\/posts\/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94\/",
          "name": "机器学习12种分类模型的性能对比"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : ""
  },
  "headline": "机器学习12种分类模型的性能对比",
  "description" : "接上回，我们已经知道了机器学习分类的本质，那么接下来就是要介绍机器学习中的分类模型的各种性能了。\n简介 我们将采用12种分类模型，分别是：GaussianNB、MultinomialNB、KNNeighbors、SVC、DecisionTree、RandomForest、GradientBoosting、LGBM、XGB、CatBoost、AdaBoost、MLP，用这些模型对数据集进行训练，然后对测试集进行预测，最后将预测结果与真实结果进行对比，得到各个模型的性能。\n数据集 我们采用以下代码生成随机分类数据：\nx,y = make_classification(n_samples=100000,n_features=3,n_classes=4,n_informative=3,n_redundant=0,random_state=50,n_clusters_per_class=1) x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1) 这里能够生成100000个样本，每个样本有3个特征，4个类别，3个特征是有效特征，0个冗余特征，随机种子为50，每个类别有1个簇。然后我们将数据集分为训练集和测试集，测试集占10%.\n由于每个样本都是一个3维向量，所以我们将数据取前2维数据将数据集可视化，如下图所示：\n评估函数 我们采用准确率(Accuracy)、精确率(Precision)、召回率(Recall)、F1值(F1-score)、R2值(R2-score)，这5个指标来评估模型的性能。\n准确率(Accuracy) 准确率是指分类正确的样本数占总样本数的比例，即：\n$Accuracy = \\frac{TP\u002bTN}{TP\u002bTN\u002bFP\u002bFN}$\n其中，TP是真正例，TN是真负例，FP是假正例，FN是假负例。\n精确率(Precision) 精确率是指分类正确的正例数占分类为正例的样本数的比例，即：\n$$ Precision = \\frac{TP}{TP\u002bFP} $$ 其中，TP是真正例，FP是假正例。\n召回率(Recall) 召回率是指分类正确的正例数占真正例的比例，即：\n$Recall = \\frac{TP}{TP\u002bFN}$\n其中，TP是真正例，FN是假负例。\nF1值(F1-score) F1值是精确率和召回率的调和平均数，即：\n$$ F1 = \\frac{2*Precision*Recall}{Precision\u002bRecall} $$ F1值越接近1，表明模型的性能越好。\nR2值(R2-score) R2值是指预测值与真实值的相关系数，即：\n$$ R2 = 1-\\frac{\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i-\\bar{y})^2} $$ 其中，$y_i$是真实值，$\\hat{y_i}$是预测值，$\\bar{y}$是真实值的均值。\nR2值越接近1，表明模型的性能越好。\n各模型表现 1. GaussianNB 其中GaussianNB的参数priors为None，var_smoothing为1e-09，表明我们没有对先验概率进行设置，而且我们对方差进行了平滑处理。\n2. MultinomialNB 其中MultinomialNB的参数alpha为1.0，fit_prior为True，class_prior为None，表明我们对先验概率进行了设置，而且我们对先验概率进行了平滑处理。\n3. KNNeighbors 其中KNNeighbors的参数n_neighbors为30，表明我们对最近邻的个数为30。\n4. SVC 5. DecisionTree 6. RandomForest ",
  "inLanguage" : "en",
  "wordCount":  386 ,
  "datePublished" : "2023-06-19T22:42:48\u002b00:00",
  "dateModified" : "2025-12-01T10:19:20\u002b00:00",
  "image" : "https:\/\/cywd123.github.io\/",
  "keywords" : [ "" ],
  "mainEntityOfPage" : "https:\/\/cywd123.github.io\/posts\/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "https:\/\/cywd123.github.io\/",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "https:\/\/cywd123.github.io\/",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>


<meta property="og:title" content="机器学习12种分类模型的性能对比" />
<meta property="og:description" content="接上回，我们已经知道了机器学习分类的本质，那么接下来就是要介绍机器学习中的分类模型的各种性能了。
简介
我们将采用12种分类模型，分别是：GaussianNB、MultinomialNB、KNNeighbors、SVC、DecisionTree、RandomForest、GradientBoosting、LGBM、XGB、CatBoost、AdaBoost、MLP，用这些模型对数据集进行训练，然后对测试集进行预测，最后将预测结果与真实结果进行对比，得到各个模型的性能。
数据集
我们采用以下代码生成随机分类数据：
x,y = make_classification(n_samples=100000,n_features=3,n_classes=4,n_informative=3,n_redundant=0,random_state=50,n_clusters_per_class=1)
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1)
这里能够生成100000个样本，每个样本有3个特征，4个类别，3个特征是有效特征，0个冗余特征，随机种子为50，每个类别有1个簇。然后我们将数据集分为训练集和测试集，测试集占10%.
由于每个样本都是一个3维向量，所以我们将数据取前2维数据将数据集可视化，如下图所示：

评估函数
我们采用准确率(Accuracy)、精确率(Precision)、召回率(Recall)、F1值(F1-score)、R2值(R2-score)，这5个指标来评估模型的性能。
准确率(Accuracy)
准确率是指分类正确的样本数占总样本数的比例，即：
$Accuracy = \frac{TP&#43;TN}{TP&#43;TN&#43;FP&#43;FN}$
其中，TP是真正例，TN是真负例，FP是假正例，FN是假负例。
精确率(Precision)
精确率是指分类正确的正例数占分类为正例的样本数的比例，即：

$$
Precision = \frac{TP}{TP&#43;FP}
$$

其中，TP是真正例，FP是假正例。
召回率(Recall)
召回率是指分类正确的正例数占真正例的比例，即：
$Recall = \frac{TP}{TP&#43;FN}$
其中，TP是真正例，FN是假负例。
F1值(F1-score)
F1值是精确率和召回率的调和平均数，即：

$$
F1 = \frac{2*Precision*Recall}{Precision&#43;Recall}
$$

F1值越接近1，表明模型的性能越好。
R2值(R2-score)
R2值是指预测值与真实值的相关系数，即：

$$
R2 = 1-\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}
$$

其中，$y_i$是真实值，$\hat{y_i}$是预测值，$\bar{y}$是真实值的均值。
R2值越接近1，表明模型的性能越好。
各模型表现
1. GaussianNB

其中GaussianNB的参数priors为None，var_smoothing为1e-09，表明我们没有对先验概率进行设置，而且我们对方差进行了平滑处理。
2. MultinomialNB

其中MultinomialNB的参数alpha为1.0，fit_prior为True，class_prior为None，表明我们对先验概率进行了设置，而且我们对先验概率进行了平滑处理。
3. KNNeighbors

其中KNNeighbors的参数n_neighbors为30，表明我们对最近邻的个数为30。
4. SVC

5. DecisionTree

6. RandomForest
">
<meta property="og:url" content="https://cywd123.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="Juiceright" />

  <meta name="twitter:title" content="机器学习12种分类模型的性能对比" />
  <meta name="twitter:description" content="接上回，我们已经知道了机器学习分类的本质，那么接下来就是要介绍机器学习中的分类模型的各种性能了。
简介
我们将采用12种分类模型，分别是：GaussianNB、MultinomialNB、KNNeighbors、SVC、DecisionTree、RandomForest、GradientBoosting、LGBM、XGB、CatBoost、AdaBoost、MLP，用这些模型对数据集进行训练，然后 …">
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="generator" content="Hugo 0.152.2">
  <link rel="alternate" href="https://cywd123.github.io/index.xml" type="application/rss+xml" title="Juiceright"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v7.0.1/css/brands.css" integrity="sha384-+eXGm6TmVoCZR1gX03US+L++L2mJIfOvp2X0+luTuEktXaF5F+e3zn0sO0HBvOyT" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v7.0.1/css/fontawesome.css" integrity="sha384-B5nyPZa7e84uCJEYHtevCdVYtQosWsbDSRtq3cy/jeSVtQW05GL8CKXO7waXsjPK" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v7.0.1/css/regular.css" integrity="sha384-V71z/A/p/DGaZAryHhiSXRQj8HdQmK2nBgL2bX23MGD+h6ZKpyxOO76f5eSXmIaj" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v7.0.1/css/solid.css" integrity="sha384-1IIShlrK5iYgvS7OAgIH3prz4RAFP7hHZ2MaLIb9l3+trFwwnw/NdCWcUHlymZDU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v7.0.1/css/svg.css" integrity="sha384-iBa7uugTOHXqQ2ngvDYJyKrPPQbd9n2P3EQe+o/0+HZe5TNvV+LO5X0lLLUrXLHP" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v7.0.1/css/svg-with-js.css" integrity="sha384-STWnS5AGz6M2YxsnGAd536EqcE2y+ktGovMMuhtXLsCBtl7lujeQj2eN4eDxa57x" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v7.0.1/css/v4-font-face.css" integrity="sha384-fH83IkgQi7kturzlxpXfNdsR8aUOcqj5HUORRVnSvJgObZkIw6aqKAaQx20IWyB/" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v7.0.1/css/v4-shims.css" integrity="sha384-cCODJHSivNBsaHei/8LC0HUD58kToSbDU+xT7Rs51BO1v/IvgT/uM0W6xMoUqKfn" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v7.0.1/css/v5-font-face.css" integrity="sha384-atSdF89mIn15dv+o2LKkDM6mCh3hwh6A7mwZ6F2it/kenqZbMu+QW/ye1OG4XSWV" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous"><link rel="stylesheet" href="https://cywd123.github.io/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" /><link rel="stylesheet" href="https://cywd123.github.io/css/syntax.css" /><link rel="stylesheet" href="https://cywd123.github.io/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">

  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://cywd123.github.io/">Juiceright</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        

        

        
      </ul>
    </div>

    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="posts-heading">
              
                <h1>机器学习12种分类模型的性能对比</h1>
              
              
                <hr class="small">
              
              
              
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <p>接上回，我们已经知道了<a href="/2023/05/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB%E7%9A%84%E6%9C%AC%E8%B4%A8/">机器学习分类的本质</a>，那么接下来就是要介绍机器学习中的分类模型的各种性能了。</p>
<h2 id="简介">简介</h2>
<p>我们将采用12种分类模型，分别是：GaussianNB、MultinomialNB、KNNeighbors、SVC、DecisionTree、RandomForest、GradientBoosting、LGBM、XGB、CatBoost、AdaBoost、MLP，用这些模型对数据集进行训练，然后对测试集进行预测，最后将预测结果与真实结果进行对比，得到各个模型的性能。</p>
<h2 id="数据集">数据集</h2>
<p>我们采用以下代码生成随机分类数据：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x,y <span style="color:#f92672">=</span> make_classification(n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">100000</span>,n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,n_classes<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,n_informative<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,n_redundant<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>,n_clusters_per_class<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>x_train,x_test,y_train,y_test <span style="color:#f92672">=</span> train_test_split(x,y,test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)
</span></span></code></pre></div><p>这里能够生成100000个样本，每个样本有3个特征，4个类别，3个特征是有效特征，0个冗余特征，随机种子为50，每个类别有1个簇。然后我们将数据集分为训练集和测试集，测试集占10%.<br>
由于每个样本都是一个3维向量，所以我们将数据取前2维数据将数据集可视化，如下图所示：<br>
<img src="/images/1741977356661.png" alt=""></p>
<h2 id="评估函数">评估函数</h2>
<p>我们采用准确率(Accuracy)、精确率(Precision)、召回率(Recall)、F1值(F1-score)、R2值(R2-score)，这5个指标来评估模型的性能。</p>
<h4 id="准确率accuracy">准确率(Accuracy)</h4>
<p>准确率是指分类正确的样本数占总样本数的比例，即：</p>
<p>$Accuracy = \frac{TP+TN}{TP+TN+FP+FN}$</p>
<p>其中，TP是真正例，TN是真负例，FP是假正例，FN是假负例。</p>
<h4 id="精确率precision">精确率(Precision)</h4>
<p>精确率是指分类正确的正例数占分类为正例的样本数的比例，即：</p>
<div>
$$
Precision = \frac{TP}{TP+FP}
$$
</div>
<p>其中，TP是真正例，FP是假正例。</p>
<h4 id="召回率recall">召回率(Recall)</h4>
<p>召回率是指分类正确的正例数占真正例的比例，即：</p>
<p>$Recall = \frac{TP}{TP+FN}$</p>
<p>其中，TP是真正例，FN是假负例。</p>
<h4 id="f1值f1-score">F1值(F1-score)</h4>
<p>F1值是精确率和召回率的调和平均数，即：</p>
<div>
$$
F1 = \frac{2*Precision*Recall}{Precision+Recall}
$$
</div>
<p>F1值越接近1，表明模型的性能越好。</p>
<h4 id="r2值r2-score">R2值(R2-score)</h4>
<p>R2值是指预测值与真实值的相关系数，即：</p>
<div>
$$
R2 = 1-\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}
$$
</div>
<p>其中，$y_i$是真实值，$\hat{y_i}$是预测值，$\bar{y}$是真实值的均值。</p>
<p>R2值越接近1，表明模型的性能越好。</p>
<h2 id="各模型表现">各模型表现</h2>
<h3 id="1-gaussiannb">1. GaussianNB</h3>
<p><img src="/images/1741977363429.png" alt=""><br>
其中GaussianNB的参数priors为None，var_smoothing为1e-09，表明我们没有对先验概率进行设置，而且我们对方差进行了平滑处理。</p>
<h3 id="2-multinomialnb">2. MultinomialNB</h3>
<p><img src="/images/1741977369592.png" alt=""><br>
其中MultinomialNB的参数alpha为1.0，fit_prior为True，class_prior为None，表明我们对先验概率进行了设置，而且我们对先验概率进行了平滑处理。</p>
<h3 id="3-knneighbors">3. KNNeighbors</h3>
<p><img src="/images/1741977374241.png" alt=""><br>
其中KNNeighbors的参数n_neighbors为30，表明我们对最近邻的个数为30。</p>
<h3 id="4-svc">4. SVC</h3>
<p><img src="/images/1741977380654.png" alt=""></p>
<h3 id="5-decisiontree">5. DecisionTree</h3>
<p><img src="/images/1741977386093.png" alt=""></p>
<h3 id="6-randomforest">6. RandomForest</h3>
<p><img src="/images/1741977391536.png" alt=""></p>
<h3 id="7-gradientboosting">7. GradientBoosting</h3>
<p><img src="/images/1741977396604.png" alt=""></p>
<h3 id="8-lgbm">8. LGBM</h3>
<p><img src="/images/1741977404010.png" alt=""></p>
<h3 id="9-xgb">9. XGB</h3>
<p><img src="/images/1741977416484.png" alt=""></p>
<h3 id="10-catboost">10. CatBoost</h3>
<p><img src="/images/1741977422098.png" alt=""></p>
<h3 id="11-adaboost">11. AdaBoost</h3>
<p><img src="/images/1741977427499.png" alt=""></p>
<h3 id="12-mlp">12. MLP</h3>
<p><img src="/images/1741977432858.png" alt=""></p>
<h2 id="总结">总结</h2>
<p>MLP(神经网络模型)在准确率、精确率、召回率、F1值、R2值上的总体表现最好，这归于神经网络模型的强大的拟合能力。<br>
其次是KNNeighbors(K近邻模型)，K近邻模型的各项指标都很高，而且更值得一提的是：K近邻模型的训练速度非常快，也是所有模型中原理最简单的。</p>
<p>总体来说：判别模型的性能要优于生成模型，这是因为判别模型的假设空间更小，所以更容易拟合数据。</p>
<p>有一点猜想：生成模型是基于贝叶斯公式的，而贝叶斯公式是基于条件概率的，而条件概率是基于联合概率的。当样本数量很大时，样本噪声也会很大，这样计算联合概率就会很困难，计算概率的准确率也会很低，所以生成模型的性能就会很差。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#加载Intel的scikit-learn加速</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearnex <span style="color:#f92672">import</span> patch_sklearn 
</span></span><span style="display:flex;"><span>patch_sklearn(global_patch<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用sklearn随机生成一个有3个分类的数据集，然后用KNN算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> make_classification
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> accuracy_score
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> r2_score <span style="color:#75715e"># R2评分，R2值越接近1，表示模型越好，越接近0，表示模型越差</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> precision_score
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> recall_score 
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> f1_score 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x,y <span style="color:#f92672">=</span> make_classification(n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">100000</span>,n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,n_classes<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,n_informative<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,n_redundant<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>,n_clusters_per_class<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#取其中的两个维度进行绘图</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Make_classification Data&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(x[:,<span style="color:#ae81ff">0</span>],x[:,<span style="color:#ae81ff">1</span>],marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>,c<span style="color:#f92672">=</span>y)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span>x_train,x_test,y_train,y_test <span style="color:#f92672">=</span> train_test_split(x,y,test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#将数据归一化，数据都是正数</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> MinMaxScaler
</span></span><span style="display:flex;"><span>scaler <span style="color:#f92672">=</span> MinMaxScaler() 
</span></span><span style="display:flex;"><span>scaler<span style="color:#f92672">.</span>fit(x_train) 
</span></span><span style="display:flex;"><span>x_train <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>transform(x_train) 
</span></span><span style="display:flex;"><span>x_test <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>transform(x_test) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accurate</span>(title):
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>title(title)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>scatter(x_test[:, <span style="color:#ae81ff">0</span>], x_test[:, <span style="color:#ae81ff">1</span>], marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>, c<span style="color:#f92672">=</span>y_predict)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>text(<span style="color:#ae81ff">0.7</span>, <span style="color:#ae81ff">0.7</span>, <span style="color:#e6db74">&#39;Accuracy:</span><span style="color:#e6db74">%.5f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> accuracy_score(y_test, y_predict)) 
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>text(<span style="color:#ae81ff">0.7</span>, <span style="color:#ae81ff">0.75</span>, <span style="color:#e6db74">&#39;Precision:</span><span style="color:#e6db74">%.5f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> precision_score(y_test, y_predict, average<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;macro&#39;</span>)) 
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>text(<span style="color:#ae81ff">0.7</span>, <span style="color:#ae81ff">0.8</span>, <span style="color:#e6db74">&#39;Recall:</span><span style="color:#e6db74">%.5f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> recall_score(y_test, y_predict, average<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;macro&#39;</span>)) 
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>text(<span style="color:#ae81ff">0.7</span>, <span style="color:#ae81ff">0.85</span>, <span style="color:#e6db74">&#39;F1 score:</span><span style="color:#e6db74">%.5f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> f1_score(y_test, y_predict, average<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;macro&#39;</span>)) 
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>text(<span style="color:#ae81ff">0.7</span>, <span style="color:#ae81ff">0.9</span>, <span style="color:#e6db74">&#39;R2 score:</span><span style="color:#e6db74">%.5f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> r2_score(y_test, y_predict))
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 用高斯贝叶斯算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.naive_bayes <span style="color:#f92672">import</span> GaussianNB, MultinomialNB
</span></span><span style="display:flex;"><span>gnb <span style="color:#f92672">=</span> GaussianNB()
</span></span><span style="display:flex;"><span>gnb<span style="color:#f92672">.</span>fit(x_train, y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> gnb<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;GaussianNB&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用多项式贝叶斯算法进行分类</span>
</span></span><span style="display:flex;"><span>mnb <span style="color:#f92672">=</span> MultinomialNB() 
</span></span><span style="display:flex;"><span>mnb<span style="color:#f92672">.</span>fit(x_train,y_train) 
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> mnb<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;MultinomialNB&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用K近邻算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.neighbors <span style="color:#f92672">import</span> KNeighborsClassifier
</span></span><span style="display:flex;"><span>knn <span style="color:#f92672">=</span> KNeighborsClassifier(n_neighbors<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>)
</span></span><span style="display:flex;"><span>knn<span style="color:#f92672">.</span>fit(x_train,y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> knn<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;KNeighborsClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用SVM算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.svm <span style="color:#f92672">import</span> SVC
</span></span><span style="display:flex;"><span>svm <span style="color:#f92672">=</span> SVC()
</span></span><span style="display:flex;"><span>svm<span style="color:#f92672">.</span>fit(x_train,y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> svm<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;SVC&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用决策树算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> DecisionTreeClassifier
</span></span><span style="display:flex;"><span>dtc <span style="color:#f92672">=</span> DecisionTreeClassifier()
</span></span><span style="display:flex;"><span>dtc<span style="color:#f92672">.</span>fit(x_train,y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> dtc<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;DecisionTreeClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用随机森林算法进行分类 </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> RandomForestClassifier 
</span></span><span style="display:flex;"><span>rfc <span style="color:#f92672">=</span> RandomForestClassifier() 
</span></span><span style="display:flex;"><span>rfc<span style="color:#f92672">.</span>fit(x_train,y_train) 
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> rfc<span style="color:#f92672">.</span>predict(x_test) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;RandomForestClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用梯度提升算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> GradientBoostingClassifier 
</span></span><span style="display:flex;"><span>gbc <span style="color:#f92672">=</span> GradientBoostingClassifier() 
</span></span><span style="display:flex;"><span>gbc<span style="color:#f92672">.</span>fit(x_train,y_train) 
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> gbc<span style="color:#f92672">.</span>predict(x_test) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;GradientBoostingClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用LightGBM算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> lightgbm <span style="color:#f92672">import</span> LGBMClassifier
</span></span><span style="display:flex;"><span>lgbmc <span style="color:#f92672">=</span> LGBMClassifier()
</span></span><span style="display:flex;"><span>lgbmc<span style="color:#f92672">.</span>fit(x_train,y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> lgbmc<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;LGBMClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用XGBoost算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> xgboost <span style="color:#f92672">import</span> XGBClassifier 
</span></span><span style="display:flex;"><span>xgbc <span style="color:#f92672">=</span> XGBClassifier() 
</span></span><span style="display:flex;"><span>xgbc<span style="color:#f92672">.</span>fit(x_train,y_train) 
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> xgbc<span style="color:#f92672">.</span>predict(x_test) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;XGBClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用CatBoost算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> catboost <span style="color:#f92672">import</span> CatBoostClassifier
</span></span><span style="display:flex;"><span>cbc <span style="color:#f92672">=</span> CatBoostClassifier()
</span></span><span style="display:flex;"><span>cbc<span style="color:#f92672">.</span>fit(x_train,y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> cbc<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;CatBoostClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用AdaBoost算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> AdaBoostClassifier
</span></span><span style="display:flex;"><span>abc <span style="color:#f92672">=</span> AdaBoostClassifier()
</span></span><span style="display:flex;"><span>abc<span style="color:#f92672">.</span>fit(x_train,y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> abc<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;AdaBoostClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用神经网络算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.neural_network <span style="color:#f92672">import</span> MLPClassifier
</span></span><span style="display:flex;"><span>mlp <span style="color:#f92672">=</span> MLPClassifier()
</span></span><span style="display:flex;"><span>mlp<span style="color:#f92672">.</span>fit(x_train,y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> mlp<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;MLPClassifier&#39;</span>)
</span></span></code></pre></div>

        

        

        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="https://cywd123.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB%E7%9A%84%E6%9C%AC%E8%B4%A8/" data-toggle="tooltip" data-placement="top" title="机器学习分类的本质">&larr; Previous Post</a>
            </li>
          
          
            <li class="next">
              <a href="https://cywd123.github.io/posts/%E5%AD%9F%E5%BE%B7%E5%B0%94%E9%9A%8F%E6%9C%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%9B%9B/" data-toggle="tooltip" data-placement="top" title="孟德尔随机化笔记（四）——药物靶点MR分析">Next Post &rarr;</a>
            </li>
          
        </ul>
      


      

    </div>
  </div>
</div>

      <footer>
  <div class="container">
    
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
        </ul>
        <p class="credits copyright text-muted">
          

          &nbsp;&bull;&nbsp;&copy;
          
            2025
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://cywd123.github.io/">Juiceright</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.152.2</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js" integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script src="https://code.jquery.com/jquery-3.7.0.slim.min.js" integrity="sha384-w5y/xIeYixWvfM+A1cEbmHPURnvyqmVg5eVENruEdDjcyRLUSNej7512JQGspFUr" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>

<script src="https://cywd123.github.io/js/main.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://cywd123.github.io/js/load-photoswipe.js"></script>










    
  </body>
</html>

